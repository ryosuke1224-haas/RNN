{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recurrent Neural Networks",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryosuke1224-haas/RNN/blob/main/Recurrent_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kTKaVK47knP"
      },
      "source": [
        "# Neural Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wl1ZLGG7wii",
        "outputId": "11c2f758-8e92-40be-a2b7-d523b1300c16"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install h5py"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.4.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.10.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py~=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.36.2)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.4.3)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.32.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.32.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.5.0,>=2.4.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.19.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (0.4.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard~=2.4->tensorflow) (1.7.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.4->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.19.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.19.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqQdG7M68HVt"
      },
      "source": [
        "s='Sing a song of sixpence,\\\n",
        "A pocket full of rye.\\\n",
        "Four and twenty blackbirds,\\\n",
        "Baked in a pie.\\\n",
        "When the pie was opened\\\n",
        "The birds began to sing;\\\n",
        "Wasn’t that a dainty dish,\\\n",
        "To set before the king.\\\n",
        "The king was in his counting house,\\\n",
        "Counting out his money;\\\n",
        "The queen was in the parlour,\\\n",
        "Eating bread and honey.\\\n",
        "The maid was in the garden,\\\n",
        "Hanging out the clothes,\\\n",
        "When down came a blackbird\\\n",
        "And pecked off her nose.'\n",
        "\n",
        "with open('rhymes.txt','w') as f:\n",
        "  f.write(s)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq7wq3fF7knV"
      },
      "source": [
        "    Sing a song of sixpence,\n",
        "    A pocket full of rye.\n",
        "    Four and twenty blackbirds,\n",
        "    Baked in a pie.\n",
        "\n",
        "    When the pie was opened\n",
        "    The birds began to sing;\n",
        "    Wasn’t that a dainty dish,\n",
        "    To set before the king.\n",
        "\n",
        "    The king was in his counting house,\n",
        "    Counting out his money;\n",
        "    The queen was in the parlour,\n",
        "    Eating bread and honey.\n",
        "\n",
        "    The maid was in the garden,\n",
        "    Hanging out the clothes,\n",
        "    When down came a blackbird\n",
        "    And pecked off her nose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjJfvv1U7knY"
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6GbMmMs7knb",
        "outputId": "0c850d3e-bede-4e76-affa-124aec84070d"
      },
      "source": [
        "#load text\n",
        "raw_text = load_doc('rhymes.txt')\n",
        "print(raw_text)\n",
        "\n",
        "# clean\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "\n",
        "# organize into sequences of characters\n",
        "length = 10 #Tried 5,10,15\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "    # select sequence of tokens\n",
        "    seq = raw_text[i-length:i+1]\n",
        "    # store\n",
        "    sequences.append(seq)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sing a song of sixpence,A pocket full of rye.Four and twenty blackbirds,Baked in a pie.When the pie was openedThe birds began to sing;Wasn’t that a dainty dish,To set before the king.The king was in his counting house,Counting out his money;The queen was in the parlour,Eating bread and honey.The maid was in the garden,Hanging out the clothes,When down came a blackbirdAnd pecked off her nose.\n",
            "Total Sequences: 384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBepEyDn7kne"
      },
      "source": [
        "# save sequences to file\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohjVbv1l7kng"
      },
      "source": [
        "# Train a Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HBgtQvY7knh"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ruXoPS7knl"
      },
      "source": [
        "# load\n",
        "\n",
        "in_filename = 'char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMndzt5v7kno",
        "outputId": "e900711c-694f-4929-c1a7-29b91afb8026"
      },
      "source": [
        "# integer encode sequences of characters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    sequences.append(encoded_seq)\n",
        "\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1jO9lYX7kny",
        "scrolled": false,
        "outputId": "582e3d6c-93ff-4259-a3b3-b80c44653db4"
      },
      "source": [
        "# Define model\n",
        "from keras.layers import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2]))) \n",
        "model.add(Dropout(0.5)) \n",
        "model.add(Dense(vocab_size, activation='softmax')) \n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "history=model.fit(X, y, epochs=150)  "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 100)               55600     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 38)                3838      \n",
            "=================================================================\n",
            "Total params: 59,438\n",
            "Trainable params: 59,438\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/150\n",
            "12/12 [==============================] - 2s 9ms/step - loss: 3.6353 - accuracy: 0.0239\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.5648 - accuracy: 0.0894\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.3728 - accuracy: 0.1416\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.3059 - accuracy: 0.1485\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.1044 - accuracy: 0.1664\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.0678 - accuracy: 0.1765\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.0846 - accuracy: 0.1491\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.0182 - accuracy: 0.1732\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.9711 - accuracy: 0.1554\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.0220 - accuracy: 0.1542\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.9830 - accuracy: 0.1714\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.0314 - accuracy: 0.1602\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.9786 - accuracy: 0.1818\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.9559 - accuracy: 0.1556\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.9203 - accuracy: 0.1935\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.9541 - accuracy: 0.1916\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.8616 - accuracy: 0.1877\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.8631 - accuracy: 0.2201\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.7975 - accuracy: 0.2134\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.8667 - accuracy: 0.1875\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.8211 - accuracy: 0.1945\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.7534 - accuracy: 0.1925\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.7174 - accuracy: 0.2468\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.6756 - accuracy: 0.2675\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.6280 - accuracy: 0.2398\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.6663 - accuracy: 0.2228\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.6634 - accuracy: 0.2369\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.6425 - accuracy: 0.2383\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.5604 - accuracy: 0.2774\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.5432 - accuracy: 0.2602\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.5472 - accuracy: 0.2637\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.4217 - accuracy: 0.2462\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.3934 - accuracy: 0.3373\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.3988 - accuracy: 0.2976\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.4374 - accuracy: 0.3067\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.3223 - accuracy: 0.3175\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.1928 - accuracy: 0.3395\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.2427 - accuracy: 0.3714\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.2065 - accuracy: 0.3395\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.1671 - accuracy: 0.3950\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.2102 - accuracy: 0.3332\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.1366 - accuracy: 0.3731\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.0256 - accuracy: 0.3776\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.0299 - accuracy: 0.4012\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.9701 - accuracy: 0.4334\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9744 - accuracy: 0.4191\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.0110 - accuracy: 0.4111\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.8669 - accuracy: 0.4324\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.9089 - accuracy: 0.4015\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.8476 - accuracy: 0.4709\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.8487 - accuracy: 0.4540\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.7460 - accuracy: 0.4929\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.7700 - accuracy: 0.4715\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6903 - accuracy: 0.4894\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.6204 - accuracy: 0.5661\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.6013 - accuracy: 0.5233\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.6966 - accuracy: 0.4787\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.5673 - accuracy: 0.5240\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4957 - accuracy: 0.5711\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.5071 - accuracy: 0.5294\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4168 - accuracy: 0.5509\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4475 - accuracy: 0.5785\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4308 - accuracy: 0.5691\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.4708 - accuracy: 0.5397\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3973 - accuracy: 0.5739\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3586 - accuracy: 0.5843\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.3299 - accuracy: 0.6315\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2661 - accuracy: 0.5773\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2204 - accuracy: 0.6259\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1560 - accuracy: 0.6787\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1424 - accuracy: 0.6543\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1307 - accuracy: 0.6625\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1299 - accuracy: 0.6932\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.0923 - accuracy: 0.6993\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0263 - accuracy: 0.6939\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.9673 - accuracy: 0.7143\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0272 - accuracy: 0.7265\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.9621 - accuracy: 0.7144\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.9381 - accuracy: 0.7248\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.9238 - accuracy: 0.7410\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.8872 - accuracy: 0.7545\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.8803 - accuracy: 0.7773\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8061 - accuracy: 0.7872\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.8305 - accuracy: 0.7552\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7957 - accuracy: 0.8024\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.7734 - accuracy: 0.7737\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7971 - accuracy: 0.7720\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7197 - accuracy: 0.8160\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6427 - accuracy: 0.8316\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6192 - accuracy: 0.8667\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6861 - accuracy: 0.8031\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6120 - accuracy: 0.8210\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5857 - accuracy: 0.8670\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6047 - accuracy: 0.8248\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6054 - accuracy: 0.8481\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5092 - accuracy: 0.8814\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5469 - accuracy: 0.8712\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5203 - accuracy: 0.8659\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4951 - accuracy: 0.8659\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4829 - accuracy: 0.8891\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4677 - accuracy: 0.9056\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5073 - accuracy: 0.8817\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4773 - accuracy: 0.8775\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4473 - accuracy: 0.8892\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4397 - accuracy: 0.9112\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.3855 - accuracy: 0.9292\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4708 - accuracy: 0.8889\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4143 - accuracy: 0.8893\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3661 - accuracy: 0.9123\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4259 - accuracy: 0.9028\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.3715 - accuracy: 0.9378\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3605 - accuracy: 0.9318\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.3831 - accuracy: 0.9109\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3453 - accuracy: 0.9185\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3445 - accuracy: 0.9174\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3954 - accuracy: 0.9132\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3542 - accuracy: 0.8990\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.2856 - accuracy: 0.9369\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3279 - accuracy: 0.9123\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3035 - accuracy: 0.9324\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2899 - accuracy: 0.9613\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2580 - accuracy: 0.9579\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.2984 - accuracy: 0.9468\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3562 - accuracy: 0.9217\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.3157 - accuracy: 0.9343\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2815 - accuracy: 0.9212\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2461 - accuracy: 0.9394\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.3089 - accuracy: 0.9144\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2597 - accuracy: 0.9507\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2558 - accuracy: 0.9502\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2309 - accuracy: 0.9427\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.2200 - accuracy: 0.9615\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2016 - accuracy: 0.9652\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2476 - accuracy: 0.9463\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2162 - accuracy: 0.9665\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.2142 - accuracy: 0.9504\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2167 - accuracy: 0.9468\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1820 - accuracy: 0.9598\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.2146 - accuracy: 0.9604\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1987 - accuracy: 0.9631\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1962 - accuracy: 0.9617\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.2345 - accuracy: 0.9522\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1986 - accuracy: 0.9498\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1899 - accuracy: 0.9589\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1789 - accuracy: 0.9738\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1666 - accuracy: 0.9772\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1896 - accuracy: 0.9742\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.1697 - accuracy: 0.9615\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1637 - accuracy: 0.9742\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.1512 - accuracy: 0.9787\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raAPR9Qp7kn1"
      },
      "source": [
        "model.save('model.h5')\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv4Q4Rkf7kn4"
      },
      "source": [
        "# Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UCWctow7kn-"
      },
      "source": [
        "from pickle import load\n",
        "from keras.models import load_model\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_chars):\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "        in_text += char\n",
        "    return in_text\n",
        "\n",
        "model = load_model('model.h5')\n",
        "mapping = load(open('mapping.pkl', 'rb'))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UhgrJYh7koB",
        "outputId": "3e94ac27-0caa-4b1f-b60f-7c0565e569a5"
      },
      "source": [
        "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
        "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
        "print(generate_seq(model, mapping, 10, 'hello worl', 20))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sing a song of sixpence,A pock\n",
            "king was in his counting house\n",
            "hello worl,,Fsrdd rrnwdttetat \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ArHWAlOtPJI",
        "outputId": "dd3d36b0-a08f-4711-be4d-b5f58ff46869"
      },
      "source": [
        "history=model.fit(X, y, epochs=150,validation_split=0.3,verbose=0) \n",
        "model.save('model.h5')\n",
        "dump(mapping, open('mapping.pkl', 'wb'))\n",
        "\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    for _ in range(n_chars):\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        yhat = model.predict_classes(encoded, verbose=0)\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "        in_text += char\n",
        "    return in_text\n",
        "\n",
        "model = load_model('model.h5')\n",
        "mapping = load(open('mapping.pkl', 'rb'))\n",
        "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
        "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
        "print(generate_seq(model, mapping, 10, 'hello worl', 20))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Sing a song of sixpence,A pock\n",
            "king was in his counting house\n",
            "hello worls,snnddnnoo teeheoos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQVDS4FLqatq"
      },
      "source": [
        "# Train a word-level model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEVVaTGLUQuJ"
      },
      "source": [
        "s=\"Venus and Adonis [But, lo! from forth a copse]\\\n",
        "William Shakespeare - 1564-1616\\\n",
        "But, lo! from forth a copse that neighbours by,\\\n",
        "A breeding jennet, lusty, young, and proud,\\\n",
        "Adonis’ trampling courser doth espy,\\\n",
        "And forth she rushes, snorts and neighs aloud;\\\n",
        "The strong-neck’d steed, being tied unto a tree,\\\n",
        "Breaketh his rein, and to her straight goes he.\\\n",
        "Imperiously he leaps, he neighs, he bounds,\\\n",
        "And now his woven girths he breaks asunder;\\\n",
        "The bearing earth with his hard hoof he wounds,\\\n",
        "Whose hollow womb resounds like heaven’s thunder;\\\n",
        "The iron bit he crushes ’tween his teeth\\\n",
        "Controlling what he was controlled with.\\\n",
        "His ears up-prick’d; his braided hanging mane\\\n",
        "Upon his compass'd crest now stand on end;\\\n",
        "His nostrils drink the air, and forth again,\\\n",
        "As from a furnace, vapours doth he send:\\\n",
        "His eye, which scornfully glisters like fire,\\\n",
        "Shows his hot courage and his high desire.\\\n",
        "Sometime her trots, as if he told the steps,\\\n",
        "With gentle majesty and modest pride;\\\n",
        "Anon he rears upright, curvets and leaps,\\\n",
        "As who should say, ’Lo! thus my strength is tried;\\\n",
        "And this I do to captivate the eye\\\n",
        "Of the fair breeder that is standing by.’\\\n",
        "What recketh he his rider’s angry stir,\\\n",
        "His flattering ’Holla,’ or his ’Stand, I say?’\\\n",
        "What cares he now for curb of pricking spur?\\\n",
        "For rich caparisons or trapping gay?\\\n",
        "He sees his love, and nothing else he sees,\\\n",
        "Nor nothing else with his proud sight agrees.\\\n",
        "Look, when a painter would surpass the life, \\\n",
        "In limning out a well-proportion’d steed,\\\n",
        "His art with nature’s workmanship at strife,\\\n",
        "As if the dead the living should exceed;\\\n",
        "So did this horse excel a common one,\\\n",
        "In shape, in courage, colour, pace and bone\\\n",
        "Round-hoof’d, short-jointed, fetlocks shag and long,\\\n",
        "Broad breast, full eye, small head, and nostril wide,\\\n",
        "High crest, short ears, straight legs and passing strong,\\\n",
        "Thin mane, thick tail, broad buttock, tender hide:\\\n",
        "Look, what a horse should have he did not lack,\\\n",
        "Save a proud rider on so proud a back.\\\n",
        "Sometimes he scuds far off, and there he stares;\\\n",
        "Anon he starts at stirring of a feather;\\\n",
        "To bid the wind a race he now prepares,\\\n",
        "And whe’r he run or fly they know not whether;\\\n",
        "For through his mane and tail the high wind sings,\\\n",
        "Fanning the hairs, who wave like feather’d wings.\\\n",
        "He looks upon his love, and neighs unto her;\\\n",
        "She answers him as if she knew his mind;\\\n",
        "Being proud, as females are, to see him woo her,\\\n",
        "She puts on outward strangeness, seems unkind,\\\n",
        "Spurns at his love and scorns the heat he feels,\\\n",
        "Beating his kind embracements with her heels.\\\n",
        "Then, like a melancholy malcontent,\\\n",
        "He vails his tail that, like a falling plume\\\n",
        "Cool shadow to his melting buttock lent:\\\n",
        "He stamps, and bites the poor flies in his fume.\\\n",
        "His love, perceiving how he is enrag’d,\\\n",
        "Grew kinder, and his fury was assuag’d.\\\n",
        "His testy master goeth about to take him;\\\n",
        "When lo! the unback’d breeder, full of fear,\\\n",
        "Jealous of catching, swiftly doth forsake him,\\\n",
        "With her the horse, and left Adonis there.\\\n",
        "As they were mad, unto the wood they hie them,\\\n",
        "Out-stripping crows that strive to over-fly them.\\\n",
        "I prophesy they death, my living sorrow,\\\n",
        "If thou encounter with the boar to-morrow.\\\n",
        "But if thou needs wilt hunt, be rul’d by me;\\\n",
        "Uncouple at the timorous flying hare,\\\n",
        "Or at the fox which lives by subtlety,\\\n",
        "Or at the roe which no encounter dare:\\\n",
        "Pursue these fearful creatures o’er the downs,\\\n",
        "And on they well-breath’d horse keep with they hounds.\\\n",
        "And when thou hast on food the purblind hare,\\\n",
        "Mark the poor wretch, to overshoot his troubles\\\n",
        "How he outruns with winds, and with what care\\\n",
        "He cranks and crosses with a thousand doubles:\\\n",
        "The many musits through the which he goes\\\n",
        "Are like a labyrinth to amaze his foes.\\\n",
        "Sometime he runs among a flock of sheep,\\\n",
        "To make the cunning hounds mistake their smell,\\\n",
        "And sometime where earth-delving conies keep,\\\n",
        "To stop the loud pursuers in their yell,\\\n",
        "And sometime sorteth with a herd of deer;\\\n",
        "Danger deviseth shifts; wit waits on fear:\\\n",
        "For there his smell with other being mingled,\\\n",
        "The hot scent-snuffing hounds are driven to doubt,\\\n",
        "Ceasing their clamorous cry till they have singled \\\n",
        "With much ado the cold fault cleanly out;\\\n",
        "Then do they spend their mouths: Echo replies,\\\n",
        "As if another chase were in the skies.\\\n",
        "By this, poor Wat, far off upon a hill,\\\n",
        "Stands on his hinder legs with listening ear,\\\n",
        "To hearken if his foes pursue him still:\\\n",
        "Anon their loud alarums he doth hear;\\\n",
        "And now his grief may be compared well\\\n",
        "To one sore sick that hears the passing-bell.\\\n",
        "Then shalt thou see the dew-bedabbled wretch\\\n",
        "Turn, and return, indenting with the way;\\\n",
        "Each envious briar his weary legs doth scratch,\\\n",
        "Each shadow makes him stop, each murmur stay:\\\n",
        "For misery is trodden on by many,\\\n",
        "And being low never reliev’d by any.\\\n",
        "Lie quietly, and hear a little more;\\\n",
        "Nay, do not struggle, for thou shalt not rise:\\\n",
        "To make thee hate the hunting of the boar,\\\n",
        "Unlike myself thou hearst’ me moralize,\\\n",
        "Applying this to that, and so to so;\\\n",
        "For love can comment upon every woe.\"\n",
        "\n",
        "with open('shakespear_venues_and_Adonis.txt','w') as f:\n",
        "  f.write(s)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tM9aSI_algdV",
        "outputId": "ea78a2a0-bee9-4d91-b5c0-1055600754cf"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras.utils as ku \n",
        "import numpy as np\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "data = open('shakespear_venues_and_Adonis.txt').read()\n",
        "texts = data.lower().split(\"\\n\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "sequences = []\n",
        "for line in texts:\n",
        " token_list = tokenizer.texts_to_sequences([line])[0]\n",
        " for i in range(1, len(token_list)):\n",
        "  n_gram_sequence = token_list[:i+1]\n",
        "  sequences.append(n_gram_sequence)\n",
        "\n",
        "max_length = max([len(x) for x in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_length, padding='pre'))\n",
        "sequences"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0, ...,   0, 118,   2],\n",
              "       [  0,   0,   0, ..., 118,   2,  58],\n",
              "       [  0,   0,   0, ...,   2,  58,  59],\n",
              "       ...,\n",
              "       [  0,   0, 118, ..., 464, 465,  53],\n",
              "       [  0, 118,   2, ..., 465,  53, 466],\n",
              "       [118,   2,  58, ...,  53, 466, 467]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbqG13floeUs"
      },
      "source": [
        "def generate_padded_sequences(sequences):\n",
        "    max_length = max([len(x) for x in sequences])\n",
        "    sequences = np.array(pad_sequences(sequences, maxlen=max_length, padding='pre'))\n",
        "    \n",
        "    predictors, label = sequences[:,:-1],sequences[:,-1]\n",
        "    label = ku.to_categorical(label, num_classes=total_words)\n",
        "    return predictors, label, max_length\n",
        "\n",
        "predictors, label, max_length = generate_padded_sequences(sequences)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6J7-UHUoRU0",
        "outputId": "0386ba19-9419-42dd-e1e0-d7c24db04fc3"
      },
      "source": [
        "def create_model(max_length, total_words):\n",
        "    input_len = max_length - 1\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Embedding(total_words, 50, input_length=input_len))\n",
        "    model.add(LSTM(200))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(total_words, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')   \n",
        "    return model\n",
        "\n",
        "model = create_model(max_length, total_words)\n",
        "model.summary()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 885, 50)           23400     \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 200)               200800    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 468)               94068     \n",
            "=================================================================\n",
            "Total params: 318,268\n",
            "Trainable params: 318,268\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrJ6vsbGl4VW",
        "outputId": "d6f5f9e4-bfdb-4d17-a7da-08bdef8bb8d0"
      },
      "source": [
        "history = model.fit(predictors, label, epochs=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "28/28 [==============================] - 53s 2s/step - loss: 6.1100\n",
            "Epoch 2/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 5.7553\n",
            "Epoch 3/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 5.6351\n",
            "Epoch 4/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 5.6256\n",
            "Epoch 5/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 5.6508\n",
            "Epoch 6/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 5.5653\n",
            "Epoch 7/100\n",
            "28/28 [==============================] - 57s 2s/step - loss: 5.5540\n",
            "Epoch 8/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 5.4555\n",
            "Epoch 9/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 5.3791\n",
            "Epoch 10/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 5.2241\n",
            "Epoch 11/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 5.1456\n",
            "Epoch 12/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 4.8628\n",
            "Epoch 13/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 4.6063\n",
            "Epoch 14/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 4.2699\n",
            "Epoch 15/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 3.9900\n",
            "Epoch 16/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 3.7684\n",
            "Epoch 17/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 3.3967\n",
            "Epoch 18/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 3.1150\n",
            "Epoch 19/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 2.7897\n",
            "Epoch 20/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 2.5105\n",
            "Epoch 21/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 2.2851\n",
            "Epoch 22/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 2.0270\n",
            "Epoch 23/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 1.8187\n",
            "Epoch 24/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 1.6449\n",
            "Epoch 25/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 1.4585\n",
            "Epoch 26/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 1.3070\n",
            "Epoch 27/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 1.2173\n",
            "Epoch 28/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 1.0479\n",
            "Epoch 29/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.9829\n",
            "Epoch 30/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.8561\n",
            "Epoch 31/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.7904\n",
            "Epoch 32/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.6927\n",
            "Epoch 33/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.6400\n",
            "Epoch 34/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.5957\n",
            "Epoch 35/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.5298\n",
            "Epoch 36/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.4770\n",
            "Epoch 37/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.4556\n",
            "Epoch 38/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.4116\n",
            "Epoch 39/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.3621\n",
            "Epoch 40/100\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.3624\n",
            "Epoch 41/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.3069\n",
            "Epoch 42/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.2972\n",
            "Epoch 43/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.2733\n",
            "Epoch 44/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.2586\n",
            "Epoch 45/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.2416\n",
            "Epoch 46/100\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.2188\n",
            "Epoch 47/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.2073\n",
            "Epoch 48/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.2005\n",
            "Epoch 49/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1866\n",
            "Epoch 50/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.1737\n",
            "Epoch 51/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1615\n",
            "Epoch 52/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1533\n",
            "Epoch 53/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1450\n",
            "Epoch 54/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1394\n",
            "Epoch 55/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1254\n",
            "Epoch 56/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1202\n",
            "Epoch 57/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1196\n",
            "Epoch 58/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1143\n",
            "Epoch 59/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.1054\n",
            "Epoch 60/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.1025\n",
            "Epoch 61/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0970\n",
            "Epoch 62/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.0959\n",
            "Epoch 63/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.0940\n",
            "Epoch 64/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.0876\n",
            "Epoch 65/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0837\n",
            "Epoch 66/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.0791\n",
            "Epoch 67/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.0798\n",
            "Epoch 68/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0739\n",
            "Epoch 69/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0697\n",
            "Epoch 70/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.0679\n",
            "Epoch 71/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0641\n",
            "Epoch 72/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0641\n",
            "Epoch 73/100\n",
            "28/28 [==============================] - 50s 2s/step - loss: 0.0615\n",
            "Epoch 74/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0581\n",
            "Epoch 75/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0550\n",
            "Epoch 76/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0558\n",
            "Epoch 77/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0497\n",
            "Epoch 78/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0515\n",
            "Epoch 79/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0488\n",
            "Epoch 80/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0465\n",
            "Epoch 81/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0458\n",
            "Epoch 82/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0420\n",
            "Epoch 83/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0424\n",
            "Epoch 84/100\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.0405\n",
            "Epoch 85/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0411\n",
            "Epoch 86/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0384\n",
            "Epoch 87/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0370\n",
            "Epoch 88/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0402\n",
            "Epoch 89/100\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.0374\n",
            "Epoch 90/100\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.0353\n",
            "Epoch 91/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0343\n",
            "Epoch 92/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0322\n",
            "Epoch 93/100\n",
            "28/28 [==============================] - 52s 2s/step - loss: 0.0298\n",
            "Epoch 94/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0317\n",
            "Epoch 95/100\n",
            "28/28 [==============================] - 51s 2s/step - loss: 0.0297\n",
            "Epoch 96/100\n",
            "19/28 [===================>..........] - ETA: 16s - loss: 0.0297"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkllPEhil93V"
      },
      "source": [
        "def generate_text(seed_text, next_words, model, max_sequence_len):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "        predicted = model.predict_classes(token_list, verbose=0)\n",
        "        \n",
        "        output_word = \"\"\n",
        "        for word,index in tokenizer.word_index.items():\n",
        "            if index == predicted:\n",
        "                output_word = word\n",
        "                break\n",
        "        seed_text += \" \"+output_word\n",
        "    return seed_text.title()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCP2fT2VmE95"
      },
      "source": [
        "#Original sentence\n",
        "print(generate_text('Then, like a melancholy malcontent',10,model,10)) \n",
        "print('Oritinal text is \"Then, like a melancholy malcontent,He vails his tail that, like a falling plume\"')\n",
        "print('===============================')\n",
        "print(generate_text('For misery is trodden on by many,',10,model,10))\n",
        "print('Oritinal text is \"For misery is trodden on by many,And being low never reliev’d by any\"')\n",
        "print('===============================')\n",
        "\n",
        "#sentence from other Shakespeare's work. One line from Macbeth\n",
        "print(generate_text('This is the sergeant,',10,model,10))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}